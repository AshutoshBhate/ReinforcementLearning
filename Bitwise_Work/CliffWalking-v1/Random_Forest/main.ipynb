{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d90eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up RLHF on CliffWalking-v1 with RANDOM FOREST ---\n",
      "--- Pre-training: Collecting random trajectories ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "--- Training Reward Model (Initial) ---\n",
      "Initial RM MSE Loss: 0.0458\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "--- Round 1/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=2000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 100       |\n",
      "|    ep_rew_mean     | -1.12e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 81        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 25        |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 2/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=4000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 100       |\n",
      "|    ep_rew_mean     | -1.04e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 222       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 9         |\n",
      "|    total_timesteps | 4096      |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 3/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=6000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.9     |\n",
      "|    ep_rew_mean     | -896     |\n",
      "| time/              |          |\n",
      "|    fps             | 261      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 4/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=8000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.3     |\n",
      "|    ep_rew_mean     | -776     |\n",
      "| time/              |          |\n",
      "|    fps             | 127      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 5/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=10000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94       |\n",
      "|    ep_rew_mean     | -620     |\n",
      "| time/              |          |\n",
      "|    fps             | 248      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 6/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=12000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | -391     |\n",
      "| time/              |          |\n",
      "|    fps             | 242      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 7/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=14000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.1     |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    fps             | 243      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 8/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=16000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.1     |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    fps             | 125      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 9/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=18000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42.5     |\n",
      "|    ep_rew_mean     | -171     |\n",
      "| time/              |          |\n",
      "|    fps             | 122      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 10/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=20000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.4     |\n",
      "|    ep_rew_mean     | -148     |\n",
      "| time/              |          |\n",
      "|    fps             | 146      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 11/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=22000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.8     |\n",
      "|    ep_rew_mean     | -143     |\n",
      "| time/              |          |\n",
      "|    fps             | 88       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 12/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=24000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27       |\n",
      "|    ep_rew_mean     | -76.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 110      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 13/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=26000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.3     |\n",
      "|    ep_rew_mean     | -68.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 185      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 14/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=28000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -71.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 140      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 15/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=30000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.1     |\n",
      "|    ep_rew_mean     | -66.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 16/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=32000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -60.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 143      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 17/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=34000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -55.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 113      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 18/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=36000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -55.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 198      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 19/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=38000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17       |\n",
      "|    ep_rew_mean     | -39.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 20/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=40000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.5     |\n",
      "|    ep_rew_mean     | -39.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 21/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=42000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.3     |\n",
      "|    ep_rew_mean     | -43      |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 22/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=44000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.2     |\n",
      "|    ep_rew_mean     | -29.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 281      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 23/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=46000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | -29.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 278      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 24/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=48000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.2     |\n",
      "|    ep_rew_mean     | -49.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Training Complete ---\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import config\n",
    "from wrappers import CliffWalkingStateWrapper, RewardPredictorWrapper\n",
    "from reward_model import RewardModel\n",
    "from teacher import Teacher\n",
    "from buffer import TrajectoryBuffer\n",
    "from train_rm import train_reward_model\n",
    "\n",
    "class RLHFDataCollectionCallback(BaseCallback):\n",
    "    def __init__(self, buffer, env, verbose=0):\n",
    "        super(RLHFDataCollectionCallback, self).__init__(verbose)\n",
    "        self.buffer = buffer\n",
    "        self._last_obs = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self._last_obs = self.training_env.reset()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals['infos'][0]\n",
    "        obs = self._last_obs if self._last_obs is not None else self.locals['new_obs'][0]\n",
    "\n",
    "        if len(obs.shape) > 1:\n",
    "            obs = obs[0]\n",
    "\n",
    "        action = self.locals['actions'][0]\n",
    "        new_obs = self.locals['new_obs'][0]\n",
    "\n",
    "        if 'original_reward' in infos:\n",
    "            real_reward = infos['original_reward']\n",
    "            self.buffer.add_step(obs, action, real_reward)\n",
    "\n",
    "        if 'episode' in infos or self.locals['dones'][0]:\n",
    "            self.buffer.finalize_episode()\n",
    "            self._last_obs = new_obs\n",
    "        else:\n",
    "            self._last_obs = new_obs\n",
    "\n",
    "        return True\n",
    "\n",
    "def inject_optimal_trajectory(env, buffer):\n",
    "    print(\"--- Injecting Optimal Demonstration ---\")\n",
    "    for _ in range(50):\n",
    "        obs, _ = env.reset()\n",
    "        actions = [0] + [1] * 11 + [2]\n",
    "\n",
    "        for action in actions:\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            real_reward = info.get('original_reward', reward)\n",
    "            buffer.add_step(obs, action, real_reward)\n",
    "            obs = next_obs\n",
    "            if terminated or truncated:\n",
    "                buffer.finalize_episode()\n",
    "\n",
    "    print(\"Optimal trajectory added to buffer (x50 copies)!\")\n",
    "\n",
    "def run_training():\n",
    "    print(f\"--- Setting up RLHF on {config.ENV_ID} with RANDOM FOREST ---\")\n",
    "\n",
    "    raw_env = gym.make(config.ENV_ID)\n",
    "    raw_env = gym.wrappers.TimeLimit(raw_env, max_episode_steps=config.MAX_STEPS)\n",
    "    env = CliffWalkingStateWrapper(raw_env)\n",
    "    env = Monitor(env)\n",
    "    env = RewardPredictorWrapper(env, reward_model=None)\n",
    "\n",
    "    eval_env_raw = gym.make(config.ENV_ID)\n",
    "    eval_env_raw = gym.wrappers.TimeLimit(eval_env_raw, max_episode_steps=config.MAX_STEPS)\n",
    "    eval_env = CliffWalkingStateWrapper(eval_env_raw)\n",
    "    eval_env = Monitor(eval_env)\n",
    "\n",
    "    demo_raw_env = gym.make(config.ENV_ID)\n",
    "    demo_env = CliffWalkingStateWrapper(demo_raw_env)\n",
    "\n",
    "    reward_model = RewardModel()\n",
    "    env.reward_model = reward_model\n",
    "\n",
    "    teacher = Teacher()\n",
    "    trajectory_buffer = TrajectoryBuffer(config.BUFFER_CAPACITY, config.SEGMENT_LENGTH)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f\"{config.TB_LOG_DIR}/rm_metrics\")\n",
    "\n",
    "    print(\"--- Pre-training: Collecting random trajectories ---\")\n",
    "    obs, _ = demo_env.reset()\n",
    "    for _ in range(config.PRETRAIN_STEPS):\n",
    "        action = demo_env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = demo_env.step(action)\n",
    "        trajectory_buffer.add_step(obs, action, reward)\n",
    "        if terminated or truncated:\n",
    "            trajectory_buffer.finalize_episode()\n",
    "            obs, _ = demo_env.reset()\n",
    "\n",
    "    inject_optimal_trajectory(demo_env, trajectory_buffer)\n",
    "\n",
    "    print(\"--- Training Reward Model (Initial) ---\")\n",
    "    pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "    if pairs:\n",
    "        initial_loss = train_reward_model(reward_model, pairs, teacher, None)\n",
    "        print(f\"Initial RM MSE Loss: {initial_loss:.4f}\")\n",
    "\n",
    "    data_callback = RLHFDataCollectionCallback(buffer=trajectory_buffer, env=env)\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=config.CHECKPOINT_FREQ,\n",
    "        save_path=config.LOG_DIR,\n",
    "        name_prefix=\"ppo_cliff\"\n",
    "    )\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=config.LOG_DIR,\n",
    "        log_path=config.LOG_DIR,\n",
    "        eval_freq=config.EVAL_FREQ,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    callback_list = CallbackList([data_callback, checkpoint_callback, eval_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config.PPO_LR,\n",
    "        n_steps=config.FEEDBACK_FREQ,\n",
    "        gamma=config.PPO_GAMMA,\n",
    "        gae_lambda=config.PPO_GAE_LAMBDA,\n",
    "        ent_coef=config.PPO_ENTROPY_COEF,\n",
    "        clip_range=config.PPO_EPS_CLIP,\n",
    "        batch_size=config.PPO_BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        tensorboard_log=config.TB_LOG_DIR,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "\n",
    "    num_rounds = config.TOTAL_TIMESTEPS // config.FEEDBACK_FREQ\n",
    "    total_timesteps_so_far = 0\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        print(f\"\\n--- Round {i+1}/{num_rounds} ---\")\n",
    "\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            inject_optimal_trajectory(demo_env, trajectory_buffer)\n",
    "\n",
    "        model.learn(\n",
    "            total_timesteps=config.FEEDBACK_FREQ,\n",
    "            callback=callback_list,\n",
    "            reset_num_timesteps=False\n",
    "        )\n",
    "        total_timesteps_so_far += config.FEEDBACK_FREQ\n",
    "\n",
    "        print(\"Training Reward Model...\")\n",
    "        pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE * 3)\n",
    "\n",
    "        if len(pairs) > 0:\n",
    "            loss = train_reward_model(reward_model, pairs, teacher, None)\n",
    "            writer.add_scalar(\"RewardModel/MSE_Loss\", loss, total_timesteps_so_far)\n",
    "\n",
    "        writer.add_scalar(\"RewardModel/Buffer_Size\", len(trajectory_buffer), total_timesteps_so_far)\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    model.save(f\"{config.LOG_DIR}/ppo_cliffwalking_final\")\n",
    "    reward_model.save(f\"{config.LOG_DIR}/reward_model_final.joblib\")\n",
    "    writer.close()\n",
    "    print(\"Models saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656958ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PbRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
