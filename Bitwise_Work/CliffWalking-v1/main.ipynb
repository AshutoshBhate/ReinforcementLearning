{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680d90eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up RLHF on CliffWalking-v1 ---\n",
      "--- Pre-training: Collecting random trajectories ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Buffer populated with 90 segments.\n",
      "--- Training Reward Model (Initial) ---\n",
      "Initial RM Loss: 0.3296\n",
      "\n",
      "--- Round 1/24 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashut\\anaconda3\\envs\\PbRL_env\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2687\n",
      "\n",
      "--- Round 2/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2281\n",
      "\n",
      "--- Round 3/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2354\n",
      "\n",
      "--- Round 4/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2375\n",
      "\n",
      "--- Round 5/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2515\n",
      "\n",
      "--- Round 6/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2638\n",
      "\n",
      "--- Round 7/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.3079\n",
      "\n",
      "--- Round 8/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.2989\n",
      "\n",
      "--- Round 9/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.3483\n",
      "\n",
      "--- Round 10/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.3551\n",
      "\n",
      "--- Round 11/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.4038\n",
      "\n",
      "--- Round 12/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.4781\n",
      "\n",
      "--- Round 13/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.4995\n",
      "\n",
      "--- Round 14/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.5466\n",
      "\n",
      "--- Round 15/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6618\n",
      "\n",
      "--- Round 16/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6684\n",
      "\n",
      "--- Round 17/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6877\n",
      "\n",
      "--- Round 18/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6986\n",
      "\n",
      "--- Round 19/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6961\n",
      "\n",
      "--- Round 20/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6947\n",
      "\n",
      "--- Round 21/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6949\n",
      "\n",
      "--- Round 22/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6950\n",
      "\n",
      "--- Round 23/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6944\n",
      "\n",
      "--- Round 24/24 ---\n",
      "Training Reward Model...\n",
      "Avg RM Loss: 0.6957\n",
      "\n",
      "--- Training Complete ---\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import config\n",
    "from wrappers import CliffWalkingStateWrapper, RewardPredictorWrapper\n",
    "from reward_model import RewardModel\n",
    "from teacher import Teacher\n",
    "from buffer import TrajectoryBuffer\n",
    "from train_rm import train_reward_model\n",
    "\n",
    "class RLHFDataCollectionCallback(BaseCallback):\n",
    "    def __init__(self, buffer, verbose=0):\n",
    "        super(RLHFDataCollectionCallback, self).__init__(verbose)\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals['infos'][0]\n",
    "        obs = self.locals['new_obs'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        \n",
    "        if 'original_reward' in infos:\n",
    "            real_reward = infos['original_reward']\n",
    "            self.buffer.add_step(obs, action, real_reward)\n",
    "            \n",
    "        if 'episode' in infos or self.locals['dones'][0]:\n",
    "            self.buffer.finalize_episode()\n",
    "            \n",
    "        return True\n",
    "    \n",
    "def inject_optimal_trajectory(env, buffer):\n",
    "    print(\"--- Injecting Optimal Demonstration ---\")\n",
    "    \n",
    "    for _ in range(50): \n",
    "        obs, _ = env.reset()\n",
    "        actions = [0] + [1]*11 + [2]\n",
    "        \n",
    "        for action in actions:\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            if 'original_reward' in info:\n",
    "                buffer.add_step(obs, action, info['original_reward'])\n",
    "            \n",
    "            obs = next_obs\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                buffer.finalize_episode()\n",
    "                \n",
    "    print(\"Optimal trajectory added to buffer (x50 copies)!\")\n",
    "    \n",
    "\n",
    "def main():\n",
    "    print(f\"--- Setting up RLHF on {config.ENV_ID} ---\")\n",
    "    \n",
    "    raw_env = gym.make(config.ENV_ID)\n",
    "    env = CliffWalkingStateWrapper(raw_env)\n",
    "    \n",
    "    reward_model = RewardModel()\n",
    "    rm_optimizer = optim.Adam(reward_model.parameters(), lr=config.RM_LR)\n",
    "    \n",
    "    env = RewardPredictorWrapper(env, reward_model)\n",
    "    \n",
    "    teacher = Teacher()\n",
    "    trajectory_buffer = TrajectoryBuffer(\n",
    "        capacity=config.BUFFER_CAPACITY,\n",
    "        segment_length=config.SEGMENT_LENGTH\n",
    "    )\n",
    "    \n",
    "    print(\"--- Pre-training: Collecting random trajectories ---\")\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(config.PRETRAIN_STEPS):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        if 'original_reward' in info:\n",
    "            trajectory_buffer.add_step(obs, action, info['original_reward'])\n",
    "            \n",
    "        if terminated or truncated:\n",
    "            trajectory_buffer.finalize_episode()\n",
    "            obs, _ = env.reset()\n",
    "            \n",
    "    inject_optimal_trajectory(env, trajectory_buffer)\n",
    "            \n",
    "    print(f\"Buffer populated with {len(trajectory_buffer)} segments.\")\n",
    "\n",
    "    print(\"--- Training Reward Model (Initial) ---\")\n",
    "    pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "    if pairs:\n",
    "        initial_loss = train_reward_model(\n",
    "            reward_model, pairs, teacher, rm_optimizer\n",
    "        )\n",
    "        print(f\"Initial RM Loss: {initial_loss:.4f}\")\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config.PPO_LR,\n",
    "        n_steps=config.FEEDBACK_FREQ,\n",
    "        gamma=config.PPO_GAMMA,\n",
    "        gae_lambda=config.PPO_GAE_LAMBDA,\n",
    "        ent_coef=config.PPO_ENTROPY_COEF,\n",
    "        clip_range=config.PPO_EPS_CLIP,\n",
    "        batch_size=config.PPO_BATCH_SIZE,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    callback = RLHFDataCollectionCallback(buffer=trajectory_buffer)\n",
    "    \n",
    "    num_rounds = config.TOTAL_TIMESTEPS // config.FEEDBACK_FREQ\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        print(f\"\\n--- Round {i+1}/{num_rounds} ---\")\n",
    "        \n",
    "        model.learn(\n",
    "            total_timesteps=config.FEEDBACK_FREQ,\n",
    "            callback=callback,\n",
    "            reset_num_timesteps=False\n",
    "        )\n",
    "        \n",
    "        print(\"Training Reward Model...\")\n",
    "        avg_loss = 0\n",
    "        training_steps = 10\n",
    "        \n",
    "        for _ in range(training_steps):\n",
    "            pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "            if len(pairs) > 0:\n",
    "                loss = train_reward_model(\n",
    "                    reward_model, pairs, teacher, rm_optimizer\n",
    "                )\n",
    "                avg_loss += loss\n",
    "        \n",
    "        print(f\"Avg RM Loss: {avg_loss/training_steps:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    \n",
    "    model.save(\"ppo_cliffwalking_rlhf\")\n",
    "    reward_model.save(\"reward_model_final.pth\")\n",
    "    print(\"Models saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1204bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PbRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
