{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680d90eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up RLHF on CliffWalking-v1 ---\n",
      "--- Pre-training: Collecting random trajectories ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "--- Training Reward Model (Initial) ---\n",
      "Initial RM Loss: 0.8721\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "--- Round 1/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=2000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 100       |\n",
      "|    ep_rew_mean     | -1.03e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 628       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 2048      |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 2/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=4000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 100       |\n",
      "|    ep_rew_mean     | -1.41e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 754       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 4096      |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 3/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=6000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.9     |\n",
      "|    ep_rew_mean     | -1.7e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 4/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=8000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 97.9      |\n",
      "|    ep_rew_mean     | -1.83e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 614       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 8192      |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 5/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=10000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.3     |\n",
      "|    ep_rew_mean     | -1.9e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 726      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 6/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=12000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 90.1      |\n",
      "|    ep_rew_mean     | -1.87e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 721       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 12288     |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 7/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=14000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 77.8      |\n",
      "|    ep_rew_mean     | -1.42e+03 |\n",
      "| time/              |           |\n",
      "|    fps             | 709       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 14336     |\n",
      "----------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 8/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=16000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.2     |\n",
      "|    ep_rew_mean     | -926     |\n",
      "| time/              |          |\n",
      "|    fps             | 756      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 9/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=18000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45.1     |\n",
      "|    ep_rew_mean     | -551     |\n",
      "| time/              |          |\n",
      "|    fps             | 684      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 10/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=20000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.6     |\n",
      "|    ep_rew_mean     | -349     |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 11/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=22000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.5     |\n",
      "|    ep_rew_mean     | -230     |\n",
      "| time/              |          |\n",
      "|    fps             | 361      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 12/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=24000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.2     |\n",
      "|    ep_rew_mean     | -159     |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 13/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=26000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.7     |\n",
      "|    ep_rew_mean     | -115     |\n",
      "| time/              |          |\n",
      "|    fps             | 189      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 14/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=28000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -149     |\n",
      "| time/              |          |\n",
      "|    fps             | 720      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 15/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=30000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.4     |\n",
      "|    ep_rew_mean     | -61.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 710      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 16/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=32000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -66.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 283      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 17/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=34000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.6     |\n",
      "|    ep_rew_mean     | -50.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 813      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 18/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=36000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.5     |\n",
      "|    ep_rew_mean     | -78.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 888      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 19/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=38000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16       |\n",
      "|    ep_rew_mean     | -58.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 895      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 20/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=40000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.2     |\n",
      "|    ep_rew_mean     | -54.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 900      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 21/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=42000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.8     |\n",
      "|    ep_rew_mean     | -43.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 933      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 22/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=44000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.2     |\n",
      "|    ep_rew_mean     | -35      |\n",
      "| time/              |          |\n",
      "|    fps             | 949      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 23/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=46000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.6     |\n",
      "|    ep_rew_mean     | -36.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 911      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 24/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=48000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.2     |\n",
      "|    ep_rew_mean     | -29      |\n",
      "| time/              |          |\n",
      "|    fps             | 879      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Training Complete ---\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import config\n",
    "from wrappers import CliffWalkingStateWrapper, RewardPredictorWrapper\n",
    "from reward_model import RewardModel\n",
    "from teacher import Teacher\n",
    "from buffer import TrajectoryBuffer\n",
    "from train_rm import train_reward_model\n",
    "\n",
    "class RLHFDataCollectionCallback(BaseCallback):\n",
    "    def __init__(self, buffer, verbose=0):\n",
    "        super(RLHFDataCollectionCallback, self).__init__(verbose)\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals['infos'][0]\n",
    "        obs = self.locals['new_obs'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "\n",
    "        if 'original_reward' in infos:\n",
    "            real_reward = infos['original_reward']\n",
    "            self.buffer.add_step(obs, action, real_reward)\n",
    "\n",
    "        if 'episode' in infos or self.locals['dones'][0]:\n",
    "            self.buffer.finalize_episode()\n",
    "        return True\n",
    "\n",
    "def inject_optimal_trajectory(env, buffer):\n",
    "    print(\"--- Injecting Optimal Demonstration ---\")\n",
    "\n",
    "    for _ in range(50):\n",
    "        obs, _ = env.reset()\n",
    "        actions = [0] + [1] * 11 + [2]\n",
    "\n",
    "        for action in actions:\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            real_reward = info.get('original_reward', reward)\n",
    "            buffer.add_step(obs, action, real_reward)\n",
    "            obs = next_obs\n",
    "\n",
    "            if terminated or truncated:\n",
    "                buffer.finalize_episode()\n",
    "\n",
    "    print(\"Optimal trajectory added to buffer (x50 copies)!\")\n",
    "\n",
    "def run_training():\n",
    "    print(f\"--- Setting up RLHF on {config.ENV_ID} ---\")\n",
    "\n",
    "    raw_env = gym.make(config.ENV_ID)\n",
    "    raw_env = gym.wrappers.TimeLimit(raw_env, max_episode_steps=config.MAX_STEPS)\n",
    "    env = CliffWalkingStateWrapper(raw_env)\n",
    "    env = Monitor(env)\n",
    "    env = RewardPredictorWrapper(env, reward_model=None)\n",
    "\n",
    "    eval_env_raw = gym.make(config.ENV_ID)\n",
    "    eval_env_raw = gym.wrappers.TimeLimit(eval_env_raw, max_episode_steps=config.MAX_STEPS)\n",
    "    eval_env = CliffWalkingStateWrapper(eval_env_raw)\n",
    "    eval_env = Monitor(eval_env)\n",
    "\n",
    "    demo_raw_env = gym.make(config.ENV_ID)\n",
    "    demo_env = CliffWalkingStateWrapper(demo_raw_env)\n",
    "\n",
    "    reward_model = RewardModel()\n",
    "    env.reward_model = reward_model\n",
    "    rm_optimizer = optim.Adam(reward_model.parameters(), lr=config.RM_LR)\n",
    "\n",
    "    teacher = Teacher()\n",
    "    trajectory_buffer = TrajectoryBuffer(config.BUFFER_CAPACITY, config.SEGMENT_LENGTH)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f\"{config.TB_LOG_DIR}/rm_metrics\")\n",
    "\n",
    "    print(\"--- Pre-training: Collecting random trajectories ---\")\n",
    "    obs, _ = demo_env.reset()\n",
    "    for _ in range(config.PRETRAIN_STEPS):\n",
    "        action = demo_env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = demo_env.step(action)\n",
    "        trajectory_buffer.add_step(obs, action, reward)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            trajectory_buffer.finalize_episode()\n",
    "            obs, _ = demo_env.reset()\n",
    "\n",
    "    inject_optimal_trajectory(demo_env, trajectory_buffer)\n",
    "\n",
    "    print(\"--- Training Reward Model (Initial) ---\")\n",
    "    pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "    if pairs:\n",
    "        initial_loss = train_reward_model(reward_model, pairs, teacher, rm_optimizer)\n",
    "        print(f\"Initial RM Loss: {initial_loss:.4f}\")\n",
    "\n",
    "    data_callback = RLHFDataCollectionCallback(buffer=trajectory_buffer)\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=config.CHECKPOINT_FREQ,\n",
    "        save_path=config.LOG_DIR,\n",
    "        name_prefix=\"ppo_cliff\"\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=config.LOG_DIR,\n",
    "        log_path=config.LOG_DIR,\n",
    "        eval_freq=config.EVAL_FREQ,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    callback_list = CallbackList([data_callback, checkpoint_callback, eval_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config.PPO_LR,\n",
    "        n_steps=config.FEEDBACK_FREQ,\n",
    "        gamma=config.PPO_GAMMA,\n",
    "        gae_lambda=config.PPO_GAE_LAMBDA,\n",
    "        ent_coef=config.PPO_ENTROPY_COEF,\n",
    "        clip_range=config.PPO_EPS_CLIP,\n",
    "        batch_size=config.PPO_BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        tensorboard_log=config.TB_LOG_DIR,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "\n",
    "    num_rounds = config.TOTAL_TIMESTEPS // config.FEEDBACK_FREQ\n",
    "    total_timesteps_so_far = 0\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        print(f\"\\n--- Round {i+1}/{num_rounds} ---\")\n",
    "\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            inject_optimal_trajectory(demo_env, trajectory_buffer)\n",
    "\n",
    "        model.learn(\n",
    "            total_timesteps=config.FEEDBACK_FREQ,\n",
    "            callback=callback_list,\n",
    "            reset_num_timesteps=False\n",
    "        )\n",
    "        total_timesteps_so_far += config.FEEDBACK_FREQ\n",
    "\n",
    "        print(\"Training Reward Model...\")\n",
    "        avg_loss = 0\n",
    "        training_steps = 10\n",
    "        for _ in range(training_steps):\n",
    "            pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "            if len(pairs) > 0:\n",
    "                loss = train_reward_model(reward_model, pairs, teacher, rm_optimizer)\n",
    "                avg_loss += loss\n",
    "\n",
    "        final_loss = avg_loss / training_steps\n",
    "\n",
    "        writer.add_scalar(\"RewardModel/Loss\", final_loss, total_timesteps_so_far)\n",
    "        writer.add_scalar(\"RewardModel/Buffer_Size\", len(trajectory_buffer), total_timesteps_so_far)\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    model.save(f\"{config.LOG_DIR}/ppo_cliffwalking_final\")\n",
    "    reward_model.save(f\"{config.LOG_DIR}/reward_model_final.pth\")\n",
    "    writer.close()\n",
    "    print(\"Models saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1204bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PbRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
