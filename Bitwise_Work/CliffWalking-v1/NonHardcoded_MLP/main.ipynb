{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680d90eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up RLHF on CliffWalking-v1 ---\n",
      "--- Pre-training: Collecting random trajectories ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "--- Training Reward Model (Initial) ---\n",
      "Initial RM Loss: 0.6701\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "--- Round 1/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=2000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -897     |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 2/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=4000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.4     |\n",
      "|    ep_rew_mean     | -920     |\n",
      "| time/              |          |\n",
      "|    fps             | 639      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 3/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=6000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.4     |\n",
      "|    ep_rew_mean     | -794     |\n",
      "| time/              |          |\n",
      "|    fps             | 624      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 4/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=8000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.2     |\n",
      "|    ep_rew_mean     | -748     |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 5/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=10000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.3     |\n",
      "|    ep_rew_mean     | -606     |\n",
      "| time/              |          |\n",
      "|    fps             | 547      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 6/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=12000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.3     |\n",
      "|    ep_rew_mean     | -463     |\n",
      "| time/              |          |\n",
      "|    fps             | 720      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 7/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=14000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 53.2     |\n",
      "|    ep_rew_mean     | -403     |\n",
      "| time/              |          |\n",
      "|    fps             | 683      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 8/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=16000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 41.3     |\n",
      "|    ep_rew_mean     | -366     |\n",
      "| time/              |          |\n",
      "|    fps             | 703      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 9/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=18000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 34.2     |\n",
      "|    ep_rew_mean     | -266     |\n",
      "| time/              |          |\n",
      "|    fps             | 765      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 10/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=20000, episode_reward=-10000.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -1e+04   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.9     |\n",
      "|    ep_rew_mean     | -190     |\n",
      "| time/              |          |\n",
      "|    fps             | 762      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 11/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=22000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26       |\n",
      "|    ep_rew_mean     | -146     |\n",
      "| time/              |          |\n",
      "|    fps             | 729      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 12/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=24000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -112     |\n",
      "| time/              |          |\n",
      "|    fps             | 746      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 13/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=26000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | -90.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 754      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 14/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=28000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -59.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 756      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 15/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=30000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 764      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 16/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=32000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -80.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 761      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 17/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=34000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.7     |\n",
      "|    ep_rew_mean     | -51.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 754      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 18/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=36000, episode_reward=-100.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -45.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 740      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 19/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=38000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -38.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 830      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 20/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=40000, episode_reward=-13.00 +/- 0.00\n",
      "Episode length: 13.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -13      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.9     |\n",
      "|    ep_rew_mean     | -40.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 831      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 21/24 ---\n",
      "--- Injecting Optimal Demonstration ---\n",
      "Optimal trajectory added to buffer (x50 copies)!\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=42000, episode_reward=-15.00 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -15      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.8     |\n",
      "|    ep_rew_mean     | -24.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 840      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 22/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=44000, episode_reward=-15.00 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -15      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -39.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 814      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 23/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=46000, episode_reward=-15.00 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -15      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -35.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 820      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Round 24/24 ---\n",
      "Logging to ./rlhf_tb_logs/PPO_0\n",
      "Eval num_timesteps=48000, episode_reward=-15.00 +/- 0.00\n",
      "Episode length: 15.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -15      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.3     |\n",
      "|    ep_rew_mean     | -41.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 824      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Training Reward Model...\n",
      "\n",
      "--- Training Complete ---\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import config\n",
    "from wrappers import CliffWalkingStateWrapper, RewardPredictorWrapper\n",
    "from reward_model import RewardModel\n",
    "from teacher import SemanticTeacher\n",
    "from buffer import TrajectoryBuffer\n",
    "from train_rm import train_reward_model\n",
    "\n",
    "class RLHFDataCollectionCallback(BaseCallback):\n",
    "    def __init__(self, buffer, verbose=0):\n",
    "        super(RLHFDataCollectionCallback, self).__init__(verbose)\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        infos = self.locals['infos'][0]\n",
    "        obs = self.locals['new_obs'][0]\n",
    "        action = self.locals['actions'][0]\n",
    "        self.buffer.add_step(obs, action)\n",
    "\n",
    "        if 'episode' in infos or self.locals['dones'][0]:\n",
    "            self.buffer.finalize_episode()\n",
    "        return True\n",
    "\n",
    "def inject_optimal_trajectory(env, buffer):\n",
    "    print(\"--- Injecting Optimal Demonstration ---\")\n",
    "\n",
    "    for _ in range(50):\n",
    "        obs, _ = env.reset()\n",
    "        actions = [0] + [1] * 11 + [2]\n",
    "\n",
    "        for action in actions:\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            buffer.add_step(obs, action)\n",
    "            obs = next_obs\n",
    "\n",
    "            if terminated or truncated:\n",
    "                buffer.finalize_episode()\n",
    "\n",
    "    print(\"Optimal trajectory added to buffer (x50 copies)!\")\n",
    "\n",
    "def run_training():\n",
    "    print(f\"--- Setting up RLHF on {config.ENV_ID} ---\")\n",
    "\n",
    "    raw_env = gym.make(config.ENV_ID)\n",
    "    raw_env = gym.wrappers.TimeLimit(raw_env, max_episode_steps=config.MAX_STEPS)\n",
    "    env = CliffWalkingStateWrapper(raw_env)\n",
    "    env = Monitor(env)\n",
    "    env = RewardPredictorWrapper(env, reward_model=None)\n",
    "\n",
    "    eval_env_raw = gym.make(config.ENV_ID)\n",
    "    eval_env_raw = gym.wrappers.TimeLimit(eval_env_raw, max_episode_steps=config.MAX_STEPS)\n",
    "    eval_env = CliffWalkingStateWrapper(eval_env_raw)\n",
    "    eval_env = Monitor(eval_env)\n",
    "\n",
    "    demo_raw_env = gym.make(config.ENV_ID)\n",
    "    demo_env = CliffWalkingStateWrapper(demo_raw_env)\n",
    "\n",
    "    reward_model = RewardModel()\n",
    "    env.reward_model = reward_model\n",
    "    rm_optimizer = optim.Adam(reward_model.parameters(), lr=config.RM_LR)\n",
    "\n",
    "    teacher = SemanticTeacher()\n",
    "    trajectory_buffer = TrajectoryBuffer(config.BUFFER_CAPACITY, config.SEGMENT_LENGTH)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=f\"{config.TB_LOG_DIR}/rm_metrics\")\n",
    "\n",
    "    print(\"--- Pre-training: Collecting random trajectories ---\")\n",
    "    obs, _ = demo_env.reset()\n",
    "    for _ in range(config.PRETRAIN_STEPS):\n",
    "        action = demo_env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = demo_env.step(action)\n",
    "        trajectory_buffer.add_step(obs, action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            trajectory_buffer.finalize_episode()\n",
    "            obs, _ = demo_env.reset()\n",
    "\n",
    "    inject_optimal_trajectory(demo_env, trajectory_buffer)\n",
    "\n",
    "    print(\"--- Training Reward Model (Initial) ---\")\n",
    "    pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "    if pairs:\n",
    "        initial_loss = train_reward_model(reward_model, pairs, teacher, rm_optimizer)\n",
    "        print(f\"Initial RM Loss: {initial_loss:.4f}\")\n",
    "\n",
    "    data_callback = RLHFDataCollectionCallback(buffer=trajectory_buffer)\n",
    "\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=config.CHECKPOINT_FREQ,\n",
    "        save_path=config.LOG_DIR,\n",
    "        name_prefix=\"ppo_cliff\"\n",
    "    )\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=config.LOG_DIR,\n",
    "        log_path=config.LOG_DIR,\n",
    "        eval_freq=config.EVAL_FREQ,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    callback_list = CallbackList([data_callback, checkpoint_callback, eval_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=config.PPO_LR,\n",
    "        n_steps=config.FEEDBACK_FREQ,\n",
    "        gamma=config.PPO_GAMMA,\n",
    "        gae_lambda=config.PPO_GAE_LAMBDA,\n",
    "        ent_coef=config.PPO_ENTROPY_COEF,\n",
    "        clip_range=config.PPO_EPS_CLIP,\n",
    "        batch_size=config.PPO_BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        tensorboard_log=config.TB_LOG_DIR,\n",
    "        device=config.DEVICE, \n",
    "        n_epochs = config.PPO_K_EPOCHS\n",
    "    )\n",
    "\n",
    "    num_rounds = config.TOTAL_TIMESTEPS // config.FEEDBACK_FREQ\n",
    "    total_timesteps_so_far = 0\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        print(f\"\\n--- Round {i+1}/{num_rounds} ---\")\n",
    "\n",
    "        if i % 5 == 0 and i > 0:\n",
    "            inject_optimal_trajectory(demo_env, trajectory_buffer)\n",
    "\n",
    "        model.learn(\n",
    "            total_timesteps=config.FEEDBACK_FREQ,\n",
    "            callback=callback_list,\n",
    "            reset_num_timesteps=False\n",
    "        )\n",
    "        total_timesteps_so_far += config.FEEDBACK_FREQ\n",
    "\n",
    "        print(\"Training Reward Model...\")\n",
    "        avg_loss = 0\n",
    "        training_steps = 10\n",
    "        for _ in range(training_steps):\n",
    "            pairs = trajectory_buffer.sample_pairs(config.RM_BATCH_SIZE)\n",
    "            if len(pairs) > 0:\n",
    "                loss = train_reward_model(reward_model, pairs, teacher, rm_optimizer)\n",
    "                avg_loss += loss\n",
    "\n",
    "        final_loss = avg_loss / training_steps\n",
    "\n",
    "        writer.add_scalar(\"RewardModel/Loss\", final_loss, total_timesteps_so_far)\n",
    "        writer.add_scalar(\"RewardModel/Buffer_Size\", len(trajectory_buffer), total_timesteps_so_far)\n",
    "\n",
    "    print(\"\\n--- Training Complete ---\")\n",
    "    model.save(f\"{config.LOG_DIR}/ppo_cliffwalking_final\")\n",
    "    reward_model.save(f\"{config.LOG_DIR}/reward_model_final.pth\")\n",
    "    writer.close()\n",
    "    print(\"Models saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1204bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PbRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
