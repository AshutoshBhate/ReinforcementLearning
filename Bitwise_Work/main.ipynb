{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f449b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashut\\anaconda3\\envs\\PbRL_env\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -3.36    |\n",
      "| time/              |          |\n",
      "|    fps             | 310      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.6748\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -1.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 384      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.6426\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 4.26     |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.5882\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.3     |\n",
      "|    ep_rew_mean     | 1.17     |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.2571\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.7     |\n",
      "|    ep_rew_mean     | 1.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 311      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.3026\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15       |\n",
      "|    ep_rew_mean     | 4.05     |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.2513\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.6     |\n",
      "|    ep_rew_mean     | 2.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 399      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.2030\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14       |\n",
      "|    ep_rew_mean     | -0.693   |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.1522\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.5     |\n",
      "|    ep_rew_mean     | 8.12     |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.2610\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.9     |\n",
      "|    ep_rew_mean     | 3.75     |\n",
      "| time/              |          |\n",
      "|    fps             | 410      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0806\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.7     |\n",
      "|    ep_rew_mean     | 7.9      |\n",
      "| time/              |          |\n",
      "|    fps             | 440      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.1653\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.7     |\n",
      "|    ep_rew_mean     | -2.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 446      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.1767\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.1     |\n",
      "|    ep_rew_mean     | 9.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 432      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.1000\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | 12.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 409      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.1697\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.3     |\n",
      "|    ep_rew_mean     | 2.54     |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0674\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.6     |\n",
      "|    ep_rew_mean     | 11.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 369      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0763\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.4     |\n",
      "|    ep_rew_mean     | 12.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 386      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.2001\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | 16.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 407      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0786\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | 13.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0518\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | 12.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 431      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0498\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | 16.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 444      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0683\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | 12.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0421\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | 9.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 413      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0685\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.9     |\n",
      "|    ep_rew_mean     | 15.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 214      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0906\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.6     |\n",
      "|    ep_rew_mean     | 18.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 193      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0903\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 309      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0268\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.9     |\n",
      "|    ep_rew_mean     | 15.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 309      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0119\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.4     |\n",
      "|    ep_rew_mean     | 19.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 209      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.1162\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.6     |\n",
      "|    ep_rew_mean     | 15.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 215      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0535\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.9     |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 205      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0361\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.2     |\n",
      "|    ep_rew_mean     | 18.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 252      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0616\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.9     |\n",
      "|    ep_rew_mean     | 20       |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0504\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.7     |\n",
      "|    ep_rew_mean     | 20.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0207\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25       |\n",
      "|    ep_rew_mean     | 20.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 119      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0356\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26       |\n",
      "|    ep_rew_mean     | 27.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0259\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.5     |\n",
      "|    ep_rew_mean     | 21.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 147      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0216\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.2     |\n",
      "|    ep_rew_mean     | 26.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 188      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0332\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.7     |\n",
      "|    ep_rew_mean     | 31.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 188      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0664\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.3     |\n",
      "|    ep_rew_mean     | 31.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 237      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.2252\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.5     |\n",
      "|    ep_rew_mean     | 19.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 301      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Reward Model Loss: 0.0336\n",
      "\n",
      "Training Complete\n",
      "Models saved\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from reward_model import RewardNetwork\n",
    "from environment import PbRLWrapper\n",
    "from buffers import TrajectoryBuffer\n",
    "from critic import AutomatedCritic\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Setup\n",
    "\n",
    "    # Hyperparameters\n",
    "    TOTAL_ITERATIONS = 40     \n",
    "    STEPS_PER_ITERATION = 2048 \n",
    "    REWARD_EPOCHS = 50      \n",
    "\n",
    "    # Initialize the components\n",
    "    reward_net = RewardNetwork(input_dim=4)\n",
    "    buffer = TrajectoryBuffer()\n",
    "    critic = AutomatedCritic()\n",
    "\n",
    "    # We create the Environment and wrap it\n",
    "    raw_env = gym.make(\"CartPole-v1\")\n",
    "    env = PbRLWrapper(raw_env, reward_net, buffer)\n",
    "\n",
    "    # Create the PPO Agent\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "   # Main Training Loop\n",
    "    \n",
    "    for iteration in range(TOTAL_ITERATIONS):\n",
    "        \n",
    "        # The agent plays in the env, wrapper overrides the reward\n",
    "        model.learn(total_timesteps=STEPS_PER_ITERATION, reset_num_timesteps=False)\n",
    "\n",
    "        # Train the Reward Model        \n",
    "        cumulative_loss = 0\n",
    "        valid_pairs = 0\n",
    "        \n",
    "        for _ in range(REWARD_EPOCHS):\n",
    "            \n",
    "            # Get two random trajectories from history\n",
    "            traj_A, traj_B = buffer.sample_pair()\n",
    "            \n",
    "            # Skip if there isn't enough data\n",
    "            if traj_A is None:\n",
    "                break\n",
    "                \n",
    "            # Ask critic which one was better\n",
    "            label = critic.judge(traj_A, traj_B)\n",
    "            \n",
    "            # Update the Reward Network to match the Critic's opinion\n",
    "            loss = reward_net.train_on_batch(traj_A, traj_B, label)\n",
    "            cumulative_loss += loss\n",
    "            valid_pairs += 1\n",
    "\n",
    "        if valid_pairs > 0:\n",
    "            avg_loss = cumulative_loss / valid_pairs\n",
    "            print(f\"Reward Model Loss: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(\"Not enough data to train Reward Model yet\")\n",
    "\n",
    "    print(\"\\nTraining Complete\")\n",
    "    \n",
    "    # Save the agent and reward model\n",
    "    model.save(\"ppo_pbrl_agent\")\n",
    "    torch.save(reward_net.state_dict(), \"reward_model.pth\")\n",
    "    print(\"Models saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59912fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PbRL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
